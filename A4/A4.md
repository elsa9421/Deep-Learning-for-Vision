## pytorch_autograd_and_nn.ipynb and pytorch_autograd_and_nn.py

This assignment has 5 parts. You will learn PyTorch on three different levels of abstraction, which will help you understand it better.

Part I, Preparation: As we always do, we will use CIFAR-10 dataset.
Part II, Barebones PyTorch: Abstraction level 1, we will work directly with the lowest-level PyTorch Tensors with autograd.
Part III, PyTorch Module API: Abstraction level 2, we will use nn.Module to define an arbitrary neural network architecture.
Part IV, PyTorch Sequential API: Abstraction level 3, we will use nn.Sequential to define a fully-connected and convolutional network very conveniently.
Part V, Residual Network: please implement your own ResNet to get a high accuracy on CIFAR-10.


## Network Visualisation

In this notebook we explore the use of image gradients for generating new images.

We will start from a convolutional neural network model which has been pretrained to perform image classification on the ImageNet dataset. 
We will use this model to define a loss function which quantifies our current unhappiness with our image, then use backpropagation to compute 
the gradient of this loss with respect to the pixels of the image. We will then keep the model fixed, and perform gradient descent on the image 
to synthesize a new image which minimizes the loss.
In this notebook we will explore three techniques for image generation:
Saliency Maps: Saliency maps are a quick way to tell which part of the image influenced the classification decision made by the network.
Adversarial Attack: We can perturb an input image so that it appears the same to humans, but will be misclassified by the pretrained network.
Class Visualization: We can synthesize an image to maximize the classification score of a particular class; this can give us some sense of what 
the network is looking for when it classifies images of that class.


## Style Transfer

In this notebook we will implement the style transfer technique from "Image Style Transfer Using Convolutional Neural Networks" (Gatys et al., CVPR 2015).
(https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf)
The general idea is to take two images, and produce a new image that reflects the content of one but the artistic "style" of the other. 
We will do this by first formulating a loss function that matches the content and style of each respective image in the feature space of a deep network, 
and then performing gradient descent on the pixels of the image itself.

The deep network we use as a feature extractor is SqueezeNet, a small model that has been trained on ImageNet. 
You could use any network, but we chose SqueezeNet here for its small size and efficiency



## RNN, LSTM, and Attention for Image Captioning
Use vanilla recurrent neural networks/LSTM/Attention-LSTM and use them it to train a model that can generate novel captions for images.

Dataset:
For this exercise we will use the 2014 release of the Microsoft COCO dataset which has become the standard testbed for image captioning.
The dataset consists of 80,000 training images and 40,000 validation images, each annotated with 5 captions written by workers on Amazon Mechanical Turk.
The dataset has been preprocessed the data saved them to a serialized data file. It contains 10,000 image-caption pairs for training and 500 for testing. 
The images have been downsampled to 112x112 for computation efficiency and captions are tokenized and numericalized, clamped to 15 words.

Architecture:
Image Feature Extraction
Here, we use MobileNet v2 for image feature extraction.
For vanilla RNN and LSTM, we use the pooled CNN feature activation. For Attention LSTM, we use the CNN feature activation map after the last convolution layer.



