## Variational Autorencoder

In this notebook, you will implement a variational autoencoder and a conditional variational autoencoder with slightly different architectures and apply them to the popular MNIST handwritten dataset. An autoencoder seeks to learn a latent representation of our training images by using unlabeled data and learning to reconstruct its inputs. The variational autoencoder extends this model by adding a probabilistic spin to the encoder and decoder, allowing us to sample from the learned distribution of the latent space to generate new images at inference time.

## GAN - Generative Adversarial Networks
Implemented 
- Vanilla GAN
-Least Squares GAN 
- DC-GAN
